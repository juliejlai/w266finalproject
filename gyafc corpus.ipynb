{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "# Tensorboard\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Helper libraries\n",
    "# from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# From sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../GYAFC_Corpus/Entertainment_Music/' ##training data only\n",
    "\n",
    "def read_file(path, file):\n",
    "    sentences = []\n",
    "    f = open(path+file, \"r\")\n",
    "    for line in f:\n",
    "        sentences.append(re.findall(r\"[\\w']+|[.,!?;-]\", line.split(\"\\n\")[0]))\n",
    "    # print(f.read())\n",
    "    f.close()\n",
    "    return sentences\n",
    "\n",
    "informal = read_file(path, file='train/informal')\n",
    "formal = read_file(path, file='train/formal')\n",
    "\n",
    "i_dev = read_file(path, file='tune/informal')\n",
    "f_dev = read_file(path, file='tune/formal.ref0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the',\n",
       "  'movie',\n",
       "  'The',\n",
       "  'In',\n",
       "  '-',\n",
       "  'Laws',\n",
       "  'not',\n",
       "  'exactly',\n",
       "  'a',\n",
       "  'holiday',\n",
       "  'movie',\n",
       "  'but',\n",
       "  'funny',\n",
       "  'and',\n",
       "  'good',\n",
       "  '!'],\n",
       " ['that', 'page', 'did', 'not', 'give', 'me', 'viroses', 'i', 'think'],\n",
       " ['of',\n",
       "  'corse',\n",
       "  'i',\n",
       "  'be',\n",
       "  'wachin',\n",
       "  'it',\n",
       "  'evry',\n",
       "  'day',\n",
       "  ',',\n",
       "  'my',\n",
       "  'fav',\n",
       "  'charachter',\n",
       "  'is',\n",
       "  'Inuasha']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "informal[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'In',\n",
       "  '-',\n",
       "  'Laws',\n",
       "  'movie',\n",
       "  \"isn't\",\n",
       "  'a',\n",
       "  'holiday',\n",
       "  'movie',\n",
       "  ',',\n",
       "  'but',\n",
       "  \"it's\",\n",
       "  'okay',\n",
       "  '.'],\n",
       " ['I', \"don't\", 'think', 'that', 'page', 'gave', 'me', 'viruses', '.'],\n",
       " ['I',\n",
       "  'watch',\n",
       "  'it',\n",
       "  'everyday',\n",
       "  ',',\n",
       "  'my',\n",
       "  'favorite',\n",
       "  'charachter',\n",
       "  'is',\n",
       "  'Inuasha',\n",
       "  '.']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formal[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    #word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of pre-standardized informal sentence:\n",
      "  ['the', 'movie', 'The', 'In', '-', 'Laws', 'not', 'exactly', 'a', 'holiday', 'movie', 'but', 'funny', 'and', 'good', '!']\n",
      "\n",
      "\n",
      "and after standardization:\n",
      "  ['<s>', 'the', 'movie', 'The', 'In', '-', 'Laws', 'not', 'exactly', 'a', 'holiday', 'movie', 'but', 'funny', 'and', 'good', '!', '<s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okcat/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "canoninformal = np.array([['<s>'] + [canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in informal ])\n",
    "print('An example of pre-standardized informal sentence:\\n  {}'.format(informal[0]))\n",
    "print('\\n\\nand after standardization:\\n  {}'.format(canoninformal[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of pre-standardized informal sentence:\n",
      "  ['The', 'In', '-', 'Laws', 'movie', \"isn't\", 'a', 'holiday', 'movie', ',', 'but', \"it's\", 'okay', '.']\n",
      "\n",
      "\n",
      "and after standardization:\n",
      "  ['<s>', 'The', 'In', '-', 'Laws', 'movie', \"isn't\", 'a', 'holiday', 'movie', ',', 'but', \"it's\", 'okay', '.', '<s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okcat/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "canonformal = np.array([['<s>'] + [canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in formal ])\n",
    "print('An example of pre-standardized informal sentence:\\n  {}'.format(formal[0]))\n",
    "print('\\n\\nand after standardization:\\n  {}'.format(canonformal[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okcat/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/okcat/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "canon_idev = np.array([['<s>'] + [canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in i_dev ])\n",
    "canon_fdev = np.array([['<s>'] + [canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in f_dev ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2877, 2877)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(canon_idev), len(canon_fdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of corpus is 55472 sentences\n",
      "Length of words in informal corpus is 55472\n"
     ]
    }
   ],
   "source": [
    "# Size of corpus\n",
    "corpus_i = np.concatenate([canoninformal ,canon_idev])\n",
    "print('Length of corpus is {} sentences'.format(len(corpus_i)))\n",
    "\n",
    "# Convert to single dimension of words \n",
    "canonwords = [ word for sentence in canoninformal for word in sentence]\n",
    "canonwords_f = [ word for sentence in canonformal for word in sentence]\n",
    "canon_i = [ word for sentence in canon_idev for word in sentence]\n",
    "canon_f = [ word for sentence in canon_fdev for word in sentence]\n",
    "\n",
    "print('Length of words in informal corpus is {}'.format(len(corpus_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40697, 40960)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(canon_i), len(canon_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vocabulary length is 10000\n"
     ]
    }
   ],
   "source": [
    "# Create the string lookup object using the 10000 most-popular words\n",
    "words_to_ids = StringLookup(max_tokens = 10000)\n",
    "\n",
    "# Process the input corpus words, creating a vocabulary / id lookup:\n",
    "words_to_ids.adapt(canonwords + canonwords_f + canon_i + canon_f)\n",
    "\n",
    "# Get vocabulary size\n",
    "V = len(words_to_ids.get_vocabulary())\n",
    "print('Extracted vocabulary length is {}'.format(V))\n",
    "\n",
    "# Also create an object to convert from ids back to words from the same vocabulary:\n",
    "ids_to_words = StringLookup(vocabulary=words_to_ids.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 747361 and 757081 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e52c67b79034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mids_labels_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ids_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ids_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# examples_per_epoch = len(corpus_ids)//(max_time+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0;32m-> 3165\u001b[0;31m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[1;32m   3166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3167\u001b[0m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0;32m--> 282\u001b[0;31m                        (self, other))\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 747361 and 757081 are not compatible"
     ]
    }
   ],
   "source": [
    "# Create training / test sets of word ids \n",
    "#corpus_ids = words_to_ids(canonwords).numpy()\n",
    "\n",
    "# Split into train (80%) dev (10%) test (10%)\n",
    "#train_ids, dev_test_ids = train_test_split(corpus_ids, train_size=0.8, random_state=42, shuffle=False)\n",
    "#dev_ids, test_ids = train_test_split(dev_test_ids, train_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "\n",
    "x_ids_train = words_to_ids(canonwords).numpy()\n",
    "y_ids_train = words_to_ids(canonwords_f).numpy()\n",
    "\n",
    "# inputs of length max_time words\n",
    "max_time = 25   # length of words per sequence\n",
    "buffer_size = 100\n",
    "batch_size = 100\n",
    "\n",
    "ids_labels_dataset = tf.data.Dataset.from_tensor_slices((x_ids_train, y_ids_train))\n",
    "# examples_per_epoch = len(corpus_ids)//(max_time+1)\n",
    "\n",
    "# Create a train sequence dimension for words.  \n",
    "sequences_train = ids_labels_dataset.batch(max_time, drop_remainder=True).shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)\n",
    "\n",
    "# Create a dataset for validating during fit\n",
    "x_dev = words_to_ids(canon_i).numpy()\n",
    "y_dev = words_to_ids(canon_f).numpy()\n",
    "ids_labels_validation = tf.data.Dataset.from_tensor_slices((x_dev, y_dev))\n",
    "sequences_val = ids_labels_validation.batch(max_time, drop_remainder=True).shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
