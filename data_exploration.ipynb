{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "# Tensorboard\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Helper libraries\n",
    "# from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# From sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in Open Subtitles Data (informal dataset)\n",
    "Split up the dataset into 500MB parts. Only loading in the smallest file (~7MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'opensub_en_split/'\n",
    "file = 'xat.txt' # the smallest file atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "f = open(path+file, \"r\")\n",
    "for line in f:\n",
    "    sentences.append(re.findall(r\"[\\w']+|[.,!?;-]\", line.split(\"\\n\")[0]))\n",
    "# print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t', 'was', 'that', '?'],\n",
       " ['Come', 'on', '.'],\n",
       " ['What', 'is', 'it', '?'],\n",
       " ['I', 'think', \"we'd\", 'better', 'get', 'out', 'of', 'here', '.'],\n",
       " ['I', 'heard', 'something', '.']]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize words -- lower case characters, convert numbers to a standard code, etc\n",
    "Also inserting a < s > character at the beginning and end of every sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from utils.py from w266 common\n",
    "\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of pre-standardized sentence:\n",
      "  ['t', 'was', 'that', '?']\n",
      "\n",
      "\n",
      "and after standardization:\n",
      "  ['<s>', 't', 'was', 'that', '?', '<s>']\n"
     ]
    }
   ],
   "source": [
    "canonsentences = np.array([['<s>'] + [canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in sentences ])\n",
    "print('An example of pre-standardized sentence:\\n  {}'.format(sentences[0]))\n",
    "print('\\n\\nand after standardization:\\n  {}'.format(canonsentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras PreProcessing: StringLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of xat.txt corpus is 232356 sentences\n",
      "Length of words in brown corpus is 2138978\n"
     ]
    }
   ],
   "source": [
    "# Size of corpus\n",
    "print('Length of xat.txt corpus is {} sentences'.format(len(canonsentences)))\n",
    "\n",
    "# Convert to single dimension of words\n",
    "canonwords = [ word for sentence in canonsentences for word in sentence]\n",
    "print('Length of words in brown corpus is {}'.format(len(canonwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vocabulary length is 10000\n"
     ]
    }
   ],
   "source": [
    "# Create the string lookup object using the 10000 most-popular words\n",
    "words_to_ids = StringLookup(max_tokens = 10000)\n",
    "\n",
    "# Process the input corpus words, creating a vocabulary / id lookup:\n",
    "words_to_ids.adapt(canonwords)\n",
    "\n",
    "# Get vocabulary size\n",
    "V = len(words_to_ids.get_vocabulary())\n",
    "print('Extracted vocabulary length is {}'.format(V))\n",
    "\n",
    "# Also create an object to convert from ids back to words from the same vocabulary:\n",
    "ids_to_words = StringLookup(vocabulary=words_to_ids.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Dataset input utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julielai/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create training / test sets of word ids \n",
    "corpus_ids = words_to_ids(canonwords).numpy()\n",
    "\n",
    "# Split into train (80%) dev (10%) test (10%)\n",
    "train_ids, dev_test_ids = train_test_split(corpus_ids, train_size=0.8, random_state=42, shuffle=False)\n",
    "\n",
    "dev_ids, test_ids = train_test_split(dev_test_ids, train_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "x_ids_train = train_ids[:-1]\n",
    "y_ids_train = train_ids[1:]\n",
    "\n",
    "# inputs of length max_time words\n",
    "max_time = 25   # length of words per sequence\n",
    "buffer_size = 100\n",
    "batch_size = 100\n",
    "\n",
    "ids_labels_dataset = tf.data.Dataset.from_tensor_slices((x_ids_train, y_ids_train))\n",
    "# examples_per_epoch = len(corpus_ids)//(max_time+1)\n",
    "\n",
    "# Create a train sequence dimension for words.  \n",
    "sequences_train = ids_labels_dataset.batch(max_time, drop_remainder=True).shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)\n",
    "\n",
    "# Create a dataset for validating during fit\n",
    "x_dev = dev_ids[:-1]\n",
    "y_dev = dev_ids[1:]\n",
    "ids_labels_validation = tf.data.Dataset.from_tensor_slices((x_dev, y_dev))\n",
    "sequences_val = ids_labels_validation.batch(max_time, drop_remainder=True).shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model. Setup tensorboard. Train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LSTM layer provides two arguments:\n",
    "#   return_state (which returns lstm_state, lstm_last_time_state, cell_state)\n",
    "#   return_sequence (which ensures that the 'lstm_state' returned object is the vector output\n",
    "#   for all time positions in the sequence.)\n",
    "#\n",
    "#   Note that for the case (return_sequence = False, return_state = True) lstm_state and lstm_last_time_state\n",
    "#   are the same tensor.\n",
    "#\n",
    "# Here is a good article illustrating the two options:\n",
    "#    https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "\n",
    "\n",
    "# Let's build a model class to instantiate our model \n",
    "# ...and more closely control training / inference behavior.\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_layers, rnn_units, hidden_activation, dropout_rate,\n",
    "                 hidden_initializer, batchnorm = True):\n",
    "        super().__init__(self)\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        # self.rnn = keras.layers.GRU(rnn_units, return_sequences = True, return_state=True, \n",
    "        #                              activation = hidden_activation, kernel_initializer = hidden_initializer, \n",
    "        #                              stateful=False)\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = []\n",
    "        self.norm = []\n",
    "        self.dropout = []\n",
    "        for i in range(n_layers):\n",
    "            self.rnn.append(tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True, \n",
    "                                                 activation = hidden_activation,\n",
    "                                                 kernel_initializer = hidden_initializer))\n",
    "            self.norm.append(tf.keras.layers.BatchNormalization())\n",
    "            self.dropout.append(tf.keras.layers.Dropout(dropout_rate))\n",
    "            \n",
    "        # self.rnn = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True,\n",
    "        #                                 activation = hidden_activation, \n",
    "        #                                 kernel_initializer = hidden_initializer)\n",
    "        \n",
    "        self.do_batchnorm = batchnorm\n",
    "    \n",
    "        # tf.keras.layers.GRU(rnn_units,\n",
    "        #                                return_sequences=True, \n",
    "        #                                return_state=True)\n",
    "        self.dense = keras.layers.Dense(vocab_size)\n",
    " \n",
    "    # You must set return_sequences=True when stacking LSTM layers so that the second LSTM layer has a three-dimensional sequence input.\n",
    "\n",
    "    def call(self, inputs, passin_states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "            \n",
    "        # In the following expression the return values are:  \n",
    "        #         x = the sequence of outputs from the layer, \n",
    "        #   state_h = the final state vector (at the last time-step)\n",
    "        #   state_c = the cell memory at the final step\n",
    "        states = []\n",
    "        for i in range(self.n_layers):\n",
    "            if passin_states is None:\n",
    "                statesi = self.rnn[i].get_initial_state(x)\n",
    "            else:\n",
    "                statesi = passin_states[i]\n",
    "            x, state_h, state_c = self.rnn[i](x, initial_state=statesi, training=training)\n",
    "            # x, state_h, state_c = self.rnn(x, initial_state=states, training=training)\n",
    "            x = self.dropout[i](x)\n",
    "            if self.do_batchnorm:\n",
    "                x = self.norm[i](x)\n",
    "            states.append((state_h, state_c))\n",
    "        \n",
    "        # Output layer outputs logits rather than softmax as we didn't specify any activation\n",
    "        x = self.dense(x, training=training)\n",
    "        \n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else: \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = 10000\n",
    "\n",
    "# The embedding dimension\n",
    "# embedding_dim = 256\n",
    "embedding_dim = 50\n",
    "\n",
    "# Number of hidden layers\n",
    "n_layers = 2\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 100\n",
    "\n",
    "hidden_activation = 'relu'\n",
    "\n",
    "hidden_initializer = 'he_uniform'\n",
    "\n",
    "# Dropout\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create model instance\n",
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    n_layers = n_layers,\n",
    "    rnn_units=rnn_units,\n",
    "    hidden_activation = hidden_activation, \n",
    "    hidden_initializer = hidden_initializer,\n",
    "    dropout_rate = dropout_rate,\n",
    "    batchnorm = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 25, 10000) # (batch_size, sequence_length, vocab_size)\n",
      "(100, 25) (100, 25)\n",
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  1010000   \n",
      "=================================================================\n",
      "Total params: 1,651,600\n",
      "Trainable params: 1,651,200\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get a feel for looking at training samples in our input Dataset\n",
    "for input_example_batch, target_example_batch in sequences_train.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
    "    print(input_example_batch.shape, target_example_batch.shape)\n",
    "\n",
    "# Print out a model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of example batch loss: ()\n",
      "Prediction shape:  (100, 25, 10000)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         9.210382\n",
      "20.0\n",
      "10000.421\n"
     ]
    }
   ],
   "source": [
    "# See the behavior of loss function, how to take mean loss over batch\n",
    "# We will use \"from_logits\" = True since our outputs are logits rather than softmax (ie, [batch,seq_len,V])\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# We can calculate an example loss using eager execution.\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print('Shape of example batch loss: {}'.format(example_batch_loss.numpy().shape))\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n",
    "\n",
    "# We know that tensorflow uses natural log (base e) for crossentropy calculation \n",
    "checkbase = loss(np.array([[[0]]]), np.array([[[0., 20, 0.]]]))\n",
    "print(checkbase.numpy())\n",
    "# Confirm that exp(mean loss ~ V)  (why?)\n",
    "# If initialization is good, each q ~ 1 / V => p ln(V) = ln(V) -> exp(ln(V)) = V !!\n",
    "# ln(x) = ln(2^log2(x)) = log2(x) * ln(2) => \n",
    "print(tf.exp(mean_loss).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', loss=loss, metrics = ['sparse_categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints\n",
    "When training a model over the course of several hours or days it is important (vital!) to setup periodic checkpoints for your model, so if something bad happens (a power outage, timeout, loss of colab resources, etc.) you will not need to start over training from scratch. This is the purpose of checkpoints.\n",
    "\n",
    "The following cell shows an example of how to set this up and use it when fitting your model. In this case we're check-pointing every epoch but you can also specify the frequency in a couple of other ways detailed in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "684/684 [==============================] - 498s 729ms/step - loss: 4.6206 - sparse_categorical_accuracy: 0.3297 - val_loss: 3.9591 - val_sparse_categorical_accuracy: 0.3596\n",
      "Epoch 2/20\n",
      "684/684 [==============================] - 393s 575ms/step - loss: 3.6735 - sparse_categorical_accuracy: 0.3749 - val_loss: 3.7728 - val_sparse_categorical_accuracy: 0.3714\n",
      "Epoch 3/20\n",
      "684/684 [==============================] - 539s 788ms/step - loss: 3.4466 - sparse_categorical_accuracy: 0.3913 - val_loss: 3.6999 - val_sparse_categorical_accuracy: 0.3781\n",
      "Epoch 4/20\n",
      "684/684 [==============================] - 547s 799ms/step - loss: 3.2988 - sparse_categorical_accuracy: 0.4044 - val_loss: 3.6619 - val_sparse_categorical_accuracy: 0.3829\n",
      "Epoch 5/20\n",
      "684/684 [==============================] - 840s 1s/step - loss: 3.1928 - sparse_categorical_accuracy: 0.4149 - val_loss: 3.6373 - val_sparse_categorical_accuracy: 0.3874\n",
      "Epoch 6/20\n",
      "684/684 [==============================] - 1042s 2s/step - loss: 3.1119 - sparse_categorical_accuracy: 0.4231 - val_loss: 3.6207 - val_sparse_categorical_accuracy: 0.3909\n",
      "Epoch 7/20\n",
      "684/684 [==============================] - 1070s 2s/step - loss: 3.0498 - sparse_categorical_accuracy: 0.4295 - val_loss: 3.6126 - val_sparse_categorical_accuracy: 0.3939\n",
      "Epoch 8/20\n",
      "684/684 [==============================] - 712s 1s/step - loss: 2.9983 - sparse_categorical_accuracy: 0.4352 - val_loss: 3.6058 - val_sparse_categorical_accuracy: 0.3949\n",
      "Epoch 9/20\n",
      "684/684 [==============================] - 672s 982ms/step - loss: 2.9568 - sparse_categorical_accuracy: 0.4399 - val_loss: 3.6030 - val_sparse_categorical_accuracy: 0.3965\n",
      "Epoch 10/20\n",
      "684/684 [==============================] - 696s 1s/step - loss: 2.9205 - sparse_categorical_accuracy: 0.4439 - val_loss: 3.5978 - val_sparse_categorical_accuracy: 0.3984\n",
      "Epoch 11/20\n",
      "684/684 [==============================] - 563s 824ms/step - loss: 2.8903 - sparse_categorical_accuracy: 0.4474 - val_loss: 3.5981 - val_sparse_categorical_accuracy: 0.3998\n",
      "Epoch 12/20\n",
      "684/684 [==============================] - 647s 946ms/step - loss: 2.8625 - sparse_categorical_accuracy: 0.4506 - val_loss: 3.5982 - val_sparse_categorical_accuracy: 0.4011\n",
      "Epoch 13/20\n",
      "684/684 [==============================] - 996s 1s/step - loss: 2.8372 - sparse_categorical_accuracy: 0.4536 - val_loss: 3.6012 - val_sparse_categorical_accuracy: 0.4027\n",
      "Epoch 14/20\n",
      "684/684 [==============================] - 1310s 2s/step - loss: 2.8169 - sparse_categorical_accuracy: 0.4560 - val_loss: 3.6021 - val_sparse_categorical_accuracy: 0.4021\n",
      "Epoch 15/20\n",
      "684/684 [==============================] - 1367s 2s/step - loss: 2.7968 - sparse_categorical_accuracy: 0.4585 - val_loss: 3.6110 - val_sparse_categorical_accuracy: 0.4040\n",
      "Epoch 16/20\n",
      "684/684 [==============================] - 967s 1s/step - loss: 2.7795 - sparse_categorical_accuracy: 0.4604 - val_loss: 3.6147 - val_sparse_categorical_accuracy: 0.4026\n",
      "Epoch 17/20\n",
      "684/684 [==============================] - 568s 831ms/step - loss: 2.7631 - sparse_categorical_accuracy: 0.4628 - val_loss: 3.6192 - val_sparse_categorical_accuracy: 0.4049\n",
      "Epoch 18/20\n",
      "684/684 [==============================] - 554s 811ms/step - loss: 2.7466 - sparse_categorical_accuracy: 0.4648 - val_loss: 3.6330 - val_sparse_categorical_accuracy: 0.4049\n",
      "Epoch 19/20\n",
      "684/684 [==============================] - 709s 1s/step - loss: 2.7337 - sparse_categorical_accuracy: 0.4661 - val_loss: 3.6385 - val_sparse_categorical_accuracy: 0.4048\n",
      "Epoch 20/20\n",
      "684/684 [==============================] - 586s 856ms/step - loss: 2.7206 - sparse_categorical_accuracy: 0.4678 - val_loss: 3.6429 - val_sparse_categorical_accuracy: 0.4047\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "EPOCHS = 20\n",
    "history = model.fit(sequences_train, \n",
    "                    validation_data = sequences_val, epochs=EPOCHS, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, ids_to_words, words_to_ids, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature=temperature\n",
    "    self.model = model\n",
    "    self.ids_to_words = ids_to_words\n",
    "    self.words_to_ids = words_to_ids\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = self.words_to_ids(['','[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices = skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(words_to_ids.get_vocabulary())]) \n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  # @tf.function\n",
    "  def generate_one_step(self, input_words, passin_states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    # input_words = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    # input_ids = self.words_to_ids(input_words).to_tensor()\n",
    "    input_words = tf.strings.split(input_words)\n",
    "    input_ids = self.words_to_ids(input_words.to_tensor())\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, word, next_word_logits] \n",
    "    predicted_logits, states =  self.model(inputs=input_ids, passin_states=passin_states, \n",
    "                                          return_state=True)\n",
    "    # Only use the prediction in the final time-position.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to words\n",
    "    predicted_words = self.ids_to_words(predicted_ids)\n",
    "\n",
    "    # Return the words and model state.\n",
    "    return predicted_words, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated language:\n",
      "hello, my name is malik from the tan city sun watching us . <s> <s> sold , they ask me . <s> <s> stop there . <s> <s> two seconds ! <s> <s> you tough , jim ? <s> <s> he did not do with me soon for a observe it later why you got the interest for a strong quarters . <s> <s> now , right ? <s> <s> it was stupid - work at another . <s> <s> i can't touch him is a nuts lot ? <s> <s> he was trash each other loves corn acts of you in your world \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.2687811851501465\n"
     ]
    }
   ],
   "source": [
    "one_step_model = OneStep(model, ids_to_words, words_to_ids)\n",
    "\n",
    "# \n",
    "start = time.time()\n",
    "states = None\n",
    "# next_word = tf.constant(['hello, my name is'])\n",
    "next_word = np.array(['hello, my name is'])\n",
    "# next_word = tf.constant([['hello, my name is'],['hello', 'my', 'name', 'is']])\n",
    "result = [next_word]\n",
    "\n",
    "for n in range(100):\n",
    "    next_word, states = one_step_model.generate_one_step(next_word, passin_states=states)\n",
    "    result.append(next_word)\n",
    "\n",
    "result = tf.strings.join(result, separator=' ')\n",
    "end = time.time()\n",
    "\n",
    "print('Generated language:')\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
