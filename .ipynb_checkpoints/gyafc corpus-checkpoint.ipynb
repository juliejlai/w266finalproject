{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import pandas as pd\n",
    "print(tf.__version__)\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend\n",
    "assert(tf.__version__.startswith(\"2.\"))\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "# Tensorboard\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Helper libraries\n",
    "# from w266_common import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# From sklearn \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, file):\n",
    "    sentences = []\n",
    "    f = open(path+file, \"r\")\n",
    "    for line in f:\n",
    "        sentences.append(re.findall(r\"[\\w']+|[.,!?;-]\", line.split(\"\\n\")[0]))\n",
    "    # print(f.read())\n",
    "    f.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104562, 104562)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path1 = '../GYAFC_Corpus/Entertainment_Music/' \n",
    "path2 = '../GYAFC_Corpus/Family_Relationships/'\n",
    "\n",
    "informal = read_file(path1, file='train/informal') + read_file(path2, file='train/informal')\n",
    "formal = read_file(path1, file='train/formal') + read_file(path2, file='train/formal')\n",
    "\n",
    "len(informal), len(formal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = ((i,f) for i in informal for f in formal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104562"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for (i,f) in zip(informal, formal):\n",
    "    data.append((i,f))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'movie',\n",
       " 'The',\n",
       " 'In',\n",
       " '-',\n",
       " 'Laws',\n",
       " 'not',\n",
       " 'exactly',\n",
       " 'a',\n",
       " 'holiday',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'funny',\n",
       " 'and',\n",
       " 'good',\n",
       " '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0] #informal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'In',\n",
       " '-',\n",
       " 'Laws',\n",
       " 'movie',\n",
       " \"isn't\",\n",
       " 'a',\n",
       " 'holiday',\n",
       " 'movie',\n",
       " ',',\n",
       " 'but',\n",
       " \"it's\",\n",
       " 'okay',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][1] #formal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>informal</th>\n",
       "      <th>formal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, movie, The, In, -, Laws, not, exactly, a...</td>\n",
       "      <td>[The, In, -, Laws, movie, isn't, a, holiday, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[that, page, did, not, give, me, viroses, i, t...</td>\n",
       "      <td>[I, don't, think, that, page, gave, me, viruse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[of, corse, i, be, wachin, it, evry, day, ,, m...</td>\n",
       "      <td>[I, watch, it, everyday, ,, my, favorite, char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[runescape, ., com, my, kids, love, it, funbra...</td>\n",
       "      <td>[Funbrain, ., com, and, runescape, ., com, are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Is, he, gay, ?, He, was, on, Late, Night, wit...</td>\n",
       "      <td>[He, was, on, the, Late, Night, show, with, Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            informal  \\\n",
       "0  [the, movie, The, In, -, Laws, not, exactly, a...   \n",
       "1  [that, page, did, not, give, me, viroses, i, t...   \n",
       "2  [of, corse, i, be, wachin, it, evry, day, ,, m...   \n",
       "3  [runescape, ., com, my, kids, love, it, funbra...   \n",
       "4  [Is, he, gay, ?, He, was, on, Late, Night, wit...   \n",
       "\n",
       "                                              formal  \n",
       "0  [The, In, -, Laws, movie, isn't, a, holiday, m...  \n",
       "1  [I, don't, think, that, page, gave, me, viruse...  \n",
       "2  [I, watch, it, everyday, ,, my, favorite, char...  \n",
       "3  [Funbrain, ., com, and, runescape, ., com, are...  \n",
       "4  [He, was, on, the, Late, Night, show, with, Co...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pd.DataFrame(data,columns=['informal','formal'])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    #word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of pre-standardized informal sentence:\n",
      "  ['the', 'movie', 'The', 'In', '-', 'Laws', 'not', 'exactly', 'a', 'holiday', 'movie', 'but', 'funny', 'and', 'good', '!']\n",
      "\n",
      "\n",
      "and after standardization:\n",
      "  ['<s>', 'the', 'movie', 'The', 'In', '-', 'Laws', 'not', 'exactly', 'a', 'holiday', 'movie', 'but', 'funny', 'and', 'good', '!', '<s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okcat/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "canoninformal = np.array([['<s>'] + [canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in informal ])\n",
    "print('An example of pre-standardized informal sentence:\\n  {}'.format(informal[0]))\n",
    "print('\\n\\nand after standardization:\\n  {}'.format(canoninformal[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of pre-standardized informal sentence:\n",
      "  ['The', 'In', '-', 'Laws', 'movie', \"isn't\", 'a', 'holiday', 'movie', ',', 'but', \"it's\", 'okay', '.']\n",
      "\n",
      "\n",
      "and after standardization:\n",
      "  ['<s>', 'The', 'In', '-', 'Laws', 'movie', \"isn't\", 'a', 'holiday', 'movie', ',', 'but', \"it's\", 'okay', '.', '<s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okcat/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "canonformal = np.array([['<s>'] + [canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in formal ])\n",
    "print('An example of pre-standardized informal sentence:\\n  {}'.format(formal[0]))\n",
    "print('\\n\\nand after standardization:\\n  {}'.format(canonformal[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okcat/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/okcat/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "canon_idev = np.array([['<s>'] + [canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in i_dev ])\n",
    "canon_fdev = np.array([['<s>'] + [canonicalize_word(word) for word in sentence] + ['<s>'] for sentence in f_dev ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2877, 2877)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(canon_idev), len(canon_fdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of corpus is 55472 sentences\n",
      "Length of words in informal corpus is 55472\n"
     ]
    }
   ],
   "source": [
    "# Size of corpus\n",
    "corpus_i = np.concatenate([canoninformal ,canon_idev])\n",
    "print('Length of corpus is {} sentences'.format(len(corpus_i)))\n",
    "\n",
    "# Convert to single dimension of words \n",
    "canonwords = [ word for sentence in canoninformal for word in sentence]\n",
    "canonwords_f = [ word for sentence in canonformal for word in sentence]\n",
    "canon_i = [ word for sentence in canon_idev for word in sentence]\n",
    "canon_f = [ word for sentence in canon_fdev for word in sentence]\n",
    "\n",
    "print('Length of words in informal corpus is {}'.format(len(corpus_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40697, 40960)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(canon_i), len(canon_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted vocabulary length is 10000\n"
     ]
    }
   ],
   "source": [
    "# Create the string lookup object using the 10000 most-popular words\n",
    "words_to_ids = StringLookup(max_tokens = 10000)\n",
    "\n",
    "# Process the input corpus words, creating a vocabulary / id lookup:\n",
    "words_to_ids.adapt(canonwords + canonwords_f + canon_i + canon_f)\n",
    "\n",
    "# Get vocabulary size\n",
    "V = len(words_to_ids.get_vocabulary())\n",
    "print('Extracted vocabulary length is {}'.format(V))\n",
    "\n",
    "# Also create an object to convert from ids back to words from the same vocabulary:\n",
    "ids_to_words = StringLookup(vocabulary=words_to_ids.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions 747361 and 757081 are not compatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e52c67b79034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mids_labels_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ids_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ids_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# examples_per_epoch = len(corpus_ids)//(max_time+1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m       batch_dim.assert_is_compatible_with(tensor_shape.Dimension(\n\u001b[0;32m-> 3165\u001b[0;31m           tensor_shape.dimension_value(t.get_shape()[0])))\n\u001b[0m\u001b[1;32m   3166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3167\u001b[0m     variant_tensor = gen_dataset_ops.tensor_slice_dataset(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0;32m--> 282\u001b[0;31m                        (self, other))\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmerge_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions 747361 and 757081 are not compatible"
     ]
    }
   ],
   "source": [
    "# Create training / test sets of word ids \n",
    "#corpus_ids = words_to_ids(canonwords).numpy()\n",
    "\n",
    "# Split into train (80%) dev (10%) test (10%)\n",
    "#train_ids, dev_test_ids = train_test_split(corpus_ids, train_size=0.8, random_state=42, shuffle=False)\n",
    "#dev_ids, test_ids = train_test_split(dev_test_ids, train_size=0.5, random_state=42, shuffle=False)\n",
    "\n",
    "\n",
    "x_ids_train = words_to_ids(canonwords).numpy()\n",
    "y_ids_train = words_to_ids(canonwords_f).numpy()\n",
    "\n",
    "# inputs of length max_time words\n",
    "max_time = 25   # length of words per sequence\n",
    "buffer_size = 100\n",
    "batch_size = 100\n",
    "\n",
    "ids_labels_dataset = tf.data.Dataset.from_tensor_slices((x_ids_train, y_ids_train))\n",
    "# examples_per_epoch = len(corpus_ids)//(max_time+1)\n",
    "\n",
    "# Create a train sequence dimension for words.  \n",
    "sequences_train = ids_labels_dataset.batch(max_time, drop_remainder=True).shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)\n",
    "\n",
    "# Create a dataset for validating during fit\n",
    "x_dev = words_to_ids(canon_i).numpy()\n",
    "y_dev = words_to_ids(canon_f).numpy()\n",
    "ids_labels_validation = tf.data.Dataset.from_tensor_slices((x_dev, y_dev))\n",
    "sequences_val = ids_labels_validation.batch(max_time, drop_remainder=True).shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
